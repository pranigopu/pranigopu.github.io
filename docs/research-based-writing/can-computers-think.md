<h1>Can Computers Think?</h1>

---

**Contents**:

- [Introduction](#introduction)
- [Defining a Computer](#defining-a-computer)
- [Automating Aspects of Thinking](#automating-aspects-of-thinking)
  - [Computation and Instruction Following](#computation-and-instruction-following)
  - [Emergent Properties of Computational Models](#emergent-properties-of-computational-models)
  - [Reasoning using a Large Language Model (LLM)](#reasoning-using-a-large-language-model-llm)
- [Conclusion](#conclusion)
- [References](#references)

---

# Introduction
Discussing whether computers can think is key to understanding the nature of computers and their functioning, which in turn is key to grasping how to interpret a computer's outputs, and thus, to uncovering how and in what ways a computer aids in the effective pursuit of cognitive and practical efficacy. It is also key to knowing whether and in what ways ethical principles applicable to sentient beings also apply to computer systems; here, note that ethical principles are not categorical imperatives but the practical principles underlying the effective long-range treatment toward an entity or being, as seen with respect to individual human well-being.

The awareness of reality is the basis of conscious survival. Such awareness starts with sensation, and the automatic integration of sensations leads to perception. At these levels, a conscious organism can grasp its environment to some extent and act accordingly in the present, either reactively or instinctually. However, the capacity for self-awareness, i.e. awareness of one's own conscious processes, allows for the selective focus of percepts, which in turn allows for abstraction and, consequently, concept-formation. Since concepts allow a consciousness to grasp facts beyond direct observation, concepts are the basis of reasoning, both inductive and deductive. "Thinking" refers to the formation and use of concepts, which often implies some form of reasoning. Hence, the titular question "can computers think?" reduces to the question "can computers form and use concepts?".

However, drawing from SEP (2022), a key albeit implicit characteristic of any act of consciousness is purposefulness; omitting purposefulness from an act of consciousness leaves no distinction between perceptiveness and reactiveness at the sensory-perceptual level, and between intentional and coincidental actions at the conceptual level. Note that a purpose or goal of an entity is a potential toward which it orients itself consistently (i.e. across time and with respect to a range of external factors) by self-generated action. Hence, a computer can only be said to think if it is being purposeful, which raises two questions: (1) Can a computer carry out processes that simulate/represent the formation and use of concepts? (2) Can a computer be purposeful by itself and thus act as an autonomous conscious entity? This essay aims to approach both these questions.

# Defining a Computer
A computer is a system that automates the processing of quantities and logical relationships. Since quantities and logical relationships are abstractions, and since conceptual-level awareness is not automatic, the processes of a computer are only meaningful when tied to a purposeful context established by a volitional consciousness (e.g. a human). However, a computer could be more loosely defined as any machine capable of emulating one or more aspects of thought (even if it is in a representational way). Even here, though, there are some questions: (1) Can a machine exactly emulate every aspect of thought (e.g. reasoning, intuition, etc.)? (2) Can even a machine that fully and exactly emulates every aspect of thought be considered conscious and capable of thinking on its own? Here, it may help to examine the extent of advancement in computer science so far and try to project its potential accordingly.

# Automating Aspects of Thinking
## Computation and Instruction Following
Quantities and their representations, i.e. numbers, are concepts which computers can and do use. Instructions also represent relationships between concepts (e.g. "add two numbers", "compare two numbers", "check this condition", etc.). However, what a computer does here is follow a predetermined structure based on given inputs, e.g. by inputting signals to predefined circuits, executing preprogrammed algorithms, etc. Any "thinking", i.e. concept-formation/use, is done by the engineer, programmer or user, not the computer. As for computer memory, note that we can record thoughts materially (e.g. through writing or digital media), but these material records, while representing thought, are not thoughts themselves (Bunge, 1956), i.e. they have no meaning apart from a thinking being.

## Emergent Properties of Computational Models
Complex computational models, such as machine learning (ML) models, display aspects of higher-level thinking, such as regression, classification and clustering. However, the functionality of the ML model is based on (1) the functional model (e.g. a neural network), (2) the performance metrics (e.g. mean squared error) and (3) the optimisation method (e.g. gradient descent). In other words, the abstraction and purpose is encoded through ML model's computations, but the computations themselves only draw meaning from the concepts and purposes of the programmer/user and are not purposeful in and of themselves.

## Reasoning using a Large Language Model (LLM)
LLMs like OpenAI's ChatGPT have advanced substantially, capable of natural language interpretation and responses in areas ranging from casual conversations to algorithmic and mathematical reasoning to literature analysis. However, this begs the question: is the AI model thinking, or is complex enough to mimic some aspects of thinking? A study by Mirzadeh et al. (2024) helps answer this question by investigating the performance of LLMs on grade-school mathematical problems, which serve as a good benchmark due to them being one of the simpler forms of higher-level reasoning. In the study, six state-of-the-art LLMs were investigated.

The key findings (applicable for each of the LLMs) are as follows: (1) the performance was sensitive to variations in the names (e.g. person names, places, foods, currencies, etc.), variations in the numerical values, and variations in both, (2) performance varied substantially with variations of the same problem, (3) performance dropped substantially with minor increases in problem difficulty, and (4) adding irrelevant information to the problems substantially reduced the performance of the LLMs (by up to 65%), and this reduction was not significantly mitigated by providing multiple examples of the same question or examples containing similar irrelevant information. These findings indicate that LLMs perform something closer to sophisticated pattern-recognition than any form of reasoning, since the abstraction of the specific content of a problem from the form of the problem is a key aspect of logical thinking.

# Conclusion
Drawing from the arguments so far bolstered by the arguments given in Noë (2024), computer systems (including advanced AI models) may at best emulate aspects of reasoning by reflecting a human intent (e.g. in algorithm design, functional model definition, performance metrics and well-defined mathematical/algorithmic approaches to performance optimisation). However, due to lacking a self-contained purpose and due to computation drawing its meaning only from the intent of the engineer/programmer/user, computer systems cannot think. ML models show that a computer can carry out processes that simulate/represent the formation and use of concepts (e.g. classification, clustering, natural language processing, etc.), but even here, ML models are only the automation of some aspect of thinking. This also applies to LLMs capable of near-human performance in various high-level tasks.

Furthermore, since consciousness is yet to be fully understood, it is unknown whether a machine can exactly emulate every aspect of thought. But if such a machine can be made and imbued with self-contained purpose, it may be said that such a machine thinks (unless there is more to consciousness than purposeful perception, abstraction and data-processing). However, it is questionable if such a machine can still be called a computer, because computers are designed as tools of automation while conceptual-level awareness is fundamentally volitional and thus non-automatic. Thus, in conclusion, computers cannot think, but it is conceivable that machines based on computer systems can.

# References
Bunge, M. (1956). 'Do Computers Think? (I)', _The British Journal for the Philosophy of Science_, 7(26), pp. 139-148. Oxford University Press.

Mirzadeh, I., Alizadeh, K., Shahrokhi, H., Tuzel, O., Bengio, S. and Farajtabar, M. (2024). GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models. _arXiv preprint arXiv:2410.05229_.

Noë, A. (2024). _Rage against the machine_. Warburton, N. (ed.). Available from: [https://aeon.co/essays/can-computers-think-no-they-cant-actually-do-anything](https://aeon.co/essays/can-computers-think-no-they-cant-actually-do-anything) \[Accessed 20 November 2024\].

Stanford Encyclopedia of Philosophy (SEP). (2022). 'Defining Critical Thinking'. _Critical Thinking_. \[online\] Available from: [https://plato.stanford.edu/entries/critical-thinking/#DefiCritThin](https://plato.stanford.edu/entries/critical-thinking/#DefiCritThin) \[Accessed 20 November 2024\].